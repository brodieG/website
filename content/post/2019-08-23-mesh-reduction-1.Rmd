---
title: "RTINI Part I: Visualizing MARTINI"
author: ~
date: '2020-01-27'
slug: mesh-reduction-1
categories: [R]
tags: [algorithms,viz]
image:
  /post/2019-08-23-mesh-reduction-1_files/images/batch-2.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "An illustrated examination of @mourner's MARTINI Javascript mesh
reduction algorithm."
output:
  blogdown::html_page:
    keep_md: true
    md_extensions: +raw_attribute
---

```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```
```{r echo=FALSE}
suppressMessages(library(ggplot2))
thm.blnk <- list(
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    axis.ticks.x=element_blank(), axis.ticks.y=element_blank(),
    panel.grid=element_blank()
  ),
  ylab(NULL),
  xlab(NULL)
)
```

# Off Course, of Course

<!-- this needs to become a shortcode -->
<img
  id='front-img'
  src='/post/2019-08-23-mesh-reduction-1_files/images/batch-2-small.png'
  data-src-big='/post/2019-08-23-mesh-reduction-1_files/images/batch-2.png'
  data-caption='Eight `volcano` approximations, flattened and stacked.'
  class='post-inset-image bgw-zoom-img'
/>

A few months ago I stumbled on Vladimir Agafonkin's ([&commat;mourner][5])
fantastic [observable notebook][6] on adapting the Will Evans etal.
Right-Triangulated Irregular Networks [(RTIN) algorithm][8] to JavaScript.  The
subject matter is interesting and the exposition superb.  I'll admit I envy the
elegance of natively interactive notebooks.

I almost left it at that, but then the irrepressible "can we do that in R"
bug bit me.  I might be one of the few folks out there actually afflicted by
this bug, but that's neither here nor there.  It doesn't help that this problem
also presents some interesting visualization challenges, so of course I veered
hopelessly off-course[^off-course] and wrote this post about it.

Why write a post about visualizing MARTINI when [&commat;mourner][5] already
does it in his?  Well, we're trying to reformulate the algorithm for
efficient use in R, and the MARTINI post focuses mostly on what the algorithm
does, a little bit on why it does it, and not so much on **how** it does it.  So
this post will focus on the last two.  Besides, I'm looking for an excuse to
`rayrender` stuff.

Why reformulate a perfectly fine JS algorithm into R, particularly when we could
just trivially port [it to C](#c-transliteration) and interface that into R?  Or
even just run it in R via [R V8][14]?  For the sport of it, duh.

# Right Triangles

The RTIN algorithm generates triangle meshes that approximate surfaces with
elevations defined on a regular grid in the x-y plane.  The beloved `volcano`
height map that comes bundled with R is exactly this type of surface.
Approximations can be constrained to an error tolerance, and here we show the
effect of specifying those tolerance on `volcano` as a percentage of its total
height:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><span
  class='bgw-img-wrap-frame bgw-inner'><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-1-small.png
  style='width: 300px;'
/><figcaption
>Full Detail (N=8,192)</figcaption></figure><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-2-small.png
  style='width: 300px;'
/><figcaption
>Tolerance: 2% (N=1,538)</figcaption></figure></span><span
  class='bgw-img-wrap-frame bgw-inner'><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-3-small.png
  style='width: 300px;'
/><figcaption
>Tolerance: 11% (N=122)</figcaption></figure><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-4-small.png
  style='width: 300px;'
/><figcaption
>Tolerance: 53% (N=13)</figcaption></figure></span></span></div>
```

We're not going to evaluate the effectiveness of this method at generating
approximation meshes as our focus is on the mechanics of the algorithm.  That
said, it's noteworthy that the "2%" approximation reduces the polygon count over
five-fold and looks good even absent shading tricks.

One of the things that pulled me into this problem is the desire to better
visualize the relationship between the original mesh and its various
approximations.  This is easily solved with lines in 2D space with
superimposition, but becomes more difficult with 2D surfaces in 3D space.
Occlusion and other artifacts from projection of the surfaces onto 2D displays
get in the way.  Possible solutions include juxtaposing in time as in
[&commat;mourner's notebook][6] with the slider, or faceting the 2D space as I
did above, but neither lets us see the details of the differences.

I ended up settling on the ["à-la-Pyramide-du-Louvre"][12] style visualization:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class='bgw-img-wrap-panel bgw-zoom-img'
  src=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-small-3.png
  data-src-big=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-3.png
  style='width: 300px;'
/><figcaption
>Tolerance: 2%</figcaption></figure><figure><img
  class='bgw-img-wrap-panel bgw-zoom-img'
  src=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-small-2.png
  data-src-big=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-2.png
  style='width: 300px;'
/><figcaption
>Tolerance: 11%</figcaption></figure><figure><img
  class='bgw-img-wrap-panel bgw-zoom-img'
  src=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-small-1.png
  data-src-big=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-1.png
  style='width: 300px;'
/><figcaption
>Tolerance: 53%</figcaption></figure></span></div>
```

Look closely (do zoom in!) particularly at the coarser approximations, and you
will see where the surface weaves in and out of the approximations.  You might
even notice that all the vertices of the approximation coincide with surface
vertices. This is because the approximated meshes are made from subsets of the
vertices of the original surface.

Unfortunately even when realistically ray traced, 3D images projected onto 2D
inevitably lose... depth.  We can compensate by presenting different
perspectives so that our brains use the parallax for additional depth
perception.  Sorry `volcano`, but I only do this to you about once a year, and
it's always for a good cause.

<!--
This horror show is so that we can force a fixed sizes for the video display
window as in auto-play mode it appears to cause the whole page to shift
seemingly at random and having a fixed height with proper aspect ratio seems to
work

Padding top trick from: https://css-tricks.com/aspect-ratio-boxes/
-->
<div class=bgw-wide-window>
<div style='width: 720px; max-width: 100%; margin: 0 auto;'>
<div style='
  width: 720px; max-width: 100%;
  overflow: hidden; padding-top: 100%; position: relative;
'>
<video
  id=louvre-rotate
  style='
    display: block; margin: 0 auto;
    width: 720px; max-width: 100%;
    position: absolute; top: 0; right: 0;
  '
  onclick='this.setAttribute("controls", "");'
  loop autoplay muted
>
<source
  src='/post/2019-08-23-mesh-reduction-1_files/images/pyramide.mp4'
  type="video/mp4"
/>
</video>
</div>
</div>
</div>

Let's flatten the elevation component of the meshes to look at their 2D
structure:

<!-- we keep this old code for reference for how to do the polyptich thing from
an Rmd hunk.
```{r flat-side-by-side, echo=FALSE, fig.width=3.125, fig.height=3.125, fig.show='hold', fig.align='default'}
old_chunk_hook <- knitr::knit_hooks$get('chunk')
knitr::knit_hooks$set(chunk=function(x, options) {
  paste0(
    '<div class=bgw-wide-window><span class=bgw-img-wrap-frame>',
    paste0(old_chunk_hook(x, options), collapse=''),
    '</span></div>'
  )
} )
source('../../static/script/mesh-viz/viz-lib.R')
vsq <- matrix(0, 65, 65)
vsq[1:65, 3:63] <- volcano[1:65,1:61]
vsq[1:65, 1:2] <- volcano[1:65, 1]
vsq[1:65, 64:65] <- volcano[1:65, 61]

elmax <- diff(range(vsq))

tol <- rev(c(50, 9.99754364417931, 1.9))
gold <- '#CCAC00'
mc <-  c(gold, 'grey50', '#CC3322')
mai <- rep(.1, 4)
errors <- rtini::rtini_error(vsq)
meshes <- lapply(tol, rtini::rtini_extract, error=errors)
de <- dim(errors)

xyz <- lapply(
  meshes, function(x) {
    ids <- rbind(matrix(unlist(x), 3L), matrix(unlist(x), 3L)[1,], NA)
    ids_to_xyz(ids, map=vsq, scale=rep(1, 3L))
} )
# switch x/y, xpd not working so we need to work-around
xpdoff <- .05
xyz <- lapply(
  xyz,
  function(w) {
    w$z <- w$x * (1 - xpdoff) + xpdoff/2
    w$x <- ((1 - w$y) * (1 - xpdoff) + xpdoff/2) * 64  + 1
    w$y <- (w$z) * 64 + 1
    w
} )
plot_tri_xy(
  xyz[[1]]$x, xyz[[1]]$y, de, lwd=1, col=mc[1], new=TRUE, mai=mai,
)
plot_tri_xy(
  xyz[[2]]$x, xyz[[2]]$y, de, lwd=1, col=mc[2], new=TRUE, mai=mai,
)
plot_tri_xy(
  xyz[[3]]$x, xyz[[3]]$y, de, lwd=1, col=mc[3], new=TRUE, mai=mai,
)
```
```{r echo=FALSE}
knitr::knit_hooks$set(chunk=old_chunk_hook)
```
```{r flat-overlay, echo=FALSE, fig.width=5.2, fig.height=5.2}
mai <- rep(.01, 4)
plot_tri_xy(
  xyz[[1]]$x, xyz[[1]]$y, de, lwd=3, col=mc[1], mai=mai, new=TRUE, xpd=NA
)
plot_tri_xy(xyz[[2]]$x, xyz[[2]]$y, de, lwd=2, col=mc[2], new=FALSE)
plot_tri_xy(xyz[[3]]$x, xyz[[3]]$y, de, lwd=.5, col=mc[3], new=FALSE)
```
-->

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/flat-1.png
  style='width: 300px;'
/><figcaption
>Tolerance: 2%</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/flat-2.png
  style='width: 300px;'
/><figcaption
>Tolerance: 11%</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/flat-3.png
  style='width: 300px;'
/><figcaption
>Tolerance: 53%</figcaption></figure></span></div>
```

In the x-y plane all the triangles are right angled isoceles triangles.  This is
where the "Right-Triangulated" part of the algorithm name comes from.  Another
important feature of this mesh approximation is that it is hierarchical.  When
we stack the three meshes we can see that each of the larger triangles in the
low fidelity meshes is exactly replaced by a set of triangles from the higher
fidelity ones:

```{=html}
<div class=bgw-wide-window>
<figure><img
  class=bgw-zoom-img
  src=/post/2019-08-23-mesh-reduction-1_files/images/stacked.png
  data-src-big=/post/2019-08-23-mesh-reduction-1_files/images/stacked-detail.png
  data-caption='Top-righthand Corner Detail of Stacked Meshes'
  style='width: 500px;'
/><figcaption>Stacked Approximation Meshes</figcaption></figure>
</div>
```

If you look closely you'll see that edges of larger triangles never cross the
faces of the smaller ones.  This is because the smaller triangles are made by
adding a vertex in the middle of the long edge of a larger triangle and
connecting it to the opposite vertex.  Every edge of the larger triangle
overlaps with an edge of the smaller triangles.  As we'll see in the next post
this creates an important constraint on how the different approximation layers
are calculated.

# Errors

We've seen that MARTINI allows us to extract approximation meshes subject to a
particular error tolerance.  At some point we have to calculate the
approximation error we test against the tolerance.  We'll illustrate this with a
minimal 3x3 elevation the sake of clarity:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/simple-mesh-fin-1.png
  style='width: 300px;'
/><figcaption
>Full Detail</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/simple-mesh-fin-2.png
  style='width: 300px;'
/><figcaption
>First Approximation</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/simple-mesh-fin-3.png
  style='width: 300px;'
/><figcaption
>Second Approximation</figcaption></figure></span></div>
```

At the most granular level (gold) there is no error as the mesh corresponds
exactly to the elevation surface.  Recall that the smaller triangles are made by
splitting larger triangles in the middle of their long edge.  At the first
level of approximation (silver) those are the midpoints of the peripheral edges.
We show the errors at those locations with the grey checked cylinders.  At the
next level of approximation (copper) we repeat the process.  This time the long
edge runs across the middle of the mesh so the error is measured dead center.

An implication of measuring error at the point we combine the smaller triangles
into bigger ones is that we never measure errors from different levels of
approximation at overlapping locations.  As such we can store the errors in a
matrix the same size as the surface, whether a simple 3 x 3 surface or a more
complex one like `volcano`[^volcano-size]:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-simple.png
  style='width: 334px;'
/><figcaption
>3 x 3 Simple Surface Errors</figcaption></figure><figure><img
  class='bgw-img-wrap-panel bgw-zoom-img'
  data-src-big=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-volc-large.png
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-volc.png
  style='width: 334px;'
/><figcaption
>65 x 65 Volcano Errors</figcaption></figure></span></div>
```

The above matrices contain the errors for every level of approximation.  We only
need to calculate the matrix once, and thereafter we can test it against
different tolerance thresholds to determine what approximation mesh to draw.

# Surface Extraction

We will play through the process of extracting meshes at different tolerance
levels.  The idea is the extract the coarsest mesh without any errors above the
threshold.  Resolving errors is easy: just break a triangle long edge such that
a vertex is added at any of the surface points for which the error is too high.
That vertex will be on the original surface and as such have no error.

Let's illustrate the process with our 3x3 error matrix, and our error tolerance
represented as a "water" level.  Errors peeking above the surface exceed the
tolerance and must be addressed:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-1.png
  style='width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-1.png
  style='width: 300px;'
/></figure></span></div>
```

For each of the three errors that exceed our threshold we split the
corresponding triangle in the middle of the long edge.  This causes us to
break up the bottom-right part of the surface into the highest detail
triangles (gold), but lets us use the first level of approximation for the top
left (grey).

As we increase our tolerance we can extend the approximation to the bottom of
the surface:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-2.png
  style='width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-2.png
  style='width: 300px;'
/></figure></span></div>
```

But if we further increase our tolerance we run into problems:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-3.png
  style='width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-3.png
  style='width: 300px;'
/></figure></span></div>
```

Breaking the right-most triangle led us to break the containing triangle
(bottom-right half) even though error in the middle of the surface is below our
threshold.  Anytime we split a triangle, all the ancestor triangles that
contain it will need to be split as well.  But that is not sufficient as
demonstrated by the remaining gap in the mesh.  We must also break up any
ancestor siblings that share an edge with the edges that are split.

You can imagine how this might complicate our mesh extraction.  We need to
check a point's error, but also the errors of any of its children, and the
children of its siblings.

One clever solution to this is to ensure that the error at any given
point is the greater of the actual error, or the errors associated with its
children.  Then, if we compute error from smallest triangles to larger ones, we
can carry over child errors to the parents:
<span id='carry-over-viz'></span>

```{=html}
<br />
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-carry-1-small.png
  style='width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-carry-2-small.png
  style='width: 300px;'
/></figure></span></div>
<br />
```

In this case we carry over the error from the right child from the first level
of approximation onto the second level.  We actually carry over the error from
all four next-generation vertices, it just so happens in this case there is only
one that has an error greater than the parent.  With this logic  our mesh
extraction can work correctly without any additional logic:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-4.png
  style='width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-4.png
  style='width: 300px;'
/></figure></span></div>
```

<!--
Use same water level as the one that caused the problem before.

Image is side by side (left/right):

* Left: error matrix, water level added
* Right: mesh approximation, complete mess
-->

# Algorithm in Action

We now have a rough idea of the algorithm.  Unfortunately I learned the hard way
that "a rough idea" is about as useful when refactoring an algorithm as it is to
know that a jar of nitroglycerin is "kinda dangerous" when you're tasked with
transporting it.  In order to safely adapt the algorithm to R we need
full mastery of the details.  Well at least I for one would have more hair left
on my head if I'd achieved some level of mastery instead of just trying to wing
it.

We will focus on the error computation over the mesh extraction algorithm as
that is the more complicated part of the process.  Its outline follows:

```{r eval=FALSE}
for (every triangle in mesh) {
  while (searching for target triangle coordinates) {
    # ... Compute coordinates of vertices `a`, `b`, and `c` ...
  }
  # ... Compute approximation error at midpoint of `a` and `b` ...
}
```

The `for` loop iterates as many times as there are triangles in the mesh.  In
our [3 x 3 grid](#three-by-three) there are six triangles (4 at the first level
of approximation, and another 2 at the next level).  In reality there are
another eight triangles at the most detailed level, but the algorithm ignores
those by definition they have no error.  For each of the checked triangles, the
`while` loop identifies their vertex coordinates `$(ax,ay)$`, `$(bx,by)$`, and
`$(cx,cy)$`, which we can use to compute the error at the middle of the long
edge, and as a result the approximation error.

Let's observe the algorithm in action, but we'll use a 5 x 5 grid this
time[^min-complex].  Regrettably we'll abandon the 3D realm for the sake of
clarity.  In this flipbook we watch the algorithm find the first triangle to get
a sense for how it works.  The visualization is generated with the help of
[`watcher`][9].  Each frame displays the state of the system _after_ the
evaluation of the highlighted line:

<div id='flipbook1' class='bgw-wide-window'></div>

When the `while` loop ends the first target triangle has been found, shown in
yellow.  In the subsequent frames the algorithm computes the location of the
midpoint `m` of the points `a` and `b` (these two always define to the long
edge), computes the approximation error at that point, and in the last frame
records it to the error matrix.  Since we're stuck in 2D we now use point size
rather than cylinder height to represent error magnitude.  

You can click/shift-click on the flipbook to step forward/back to see clearly
what each step of the algorithm is doing.  We stopped the flipbook before the
child error propagation step as at the first level there are no child errors to
propagate[^no-child-error].

What happens next is pretty remarkable.  At the top of the loop we're about to
repeat we have:

```{r eval=FALSE}
for (i in (nTriangles - 1):0) {
  id <- i + 2L
  # ... rest of code omitted
```

This innocent-seeming reset of `id` will ensure that the rest of the logic
finds the next smallest triangle, and the next after that, and so forth:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/mesh-anim-4-abreast.png'
/>

At this point we've tiled the whole surface at the most granular approximation
level, but we need to repeat the process for the next approximation level.
Remarkably the smaller initial `id` values cause the `while` loop to exit
earlier at the next larger triangle size.  Here is another flipbook showing the
search for the second triangle of the next approximation
level[^second-triangle]:

<div id='flipbook2' class='bgw-wide-window'></div>

The last few frames show how the error from the children elements (labeled
`lc` and `rc` for "left child" and "right child") carry over into the current
target point.  As we [saw earlier](#carry-over-viz) this ensures there are no
gaps in our final mesh.  Each error point will have errors carried over from up
to four neighboring children.  For the above point the error from the remaining
children is carried over a few hundred steps later when the adjoining triangle
is processed:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/mesh-anim-2-abreast.png'
/>

In this case it the additional child errors are smaller so they do not change
the outcome, but it is worth noting that each long edge midpoint may have up to
four child errors added to it.

Here is the whole thing at 60fps:

<video
  id=mesh-anim
  style='display: block; margin: 0 auto;'
  controls loop
>
<source
  src='/post/2019-08-23-mesh-reduction-1_files/images/out.mp4'
  type="video/mp4"
/>
</video>

# R's Kryptonite

For the algorithm visualization we "transliterated"[^transliterated] the
JavaScript algorithm into R.  While this works, it does so slowly.  Compare the
timings of our R function, the JavaScript version, and finally a [C
version](c-version):

```{r rtin-timings, echo=FALSE}
dat <- data.frame(Language=c('R', 'JS', 'C'), Time=c(2500,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

I was surprised by how well JS stacks up relative to C.  Certainly a ~3x gap is
nothing to sneeze at, but I expected it to be bigger given JS is
interpreted[^timings-note].  On the other hand R does terribly.  This is
expected as R is not designed to run tight loops on scalar values.  We could
take some steps to get the loop to run a little faster, but not to run fast
enough to run in real-time at scale.

# Let's Vectorize the Absolute $#!+ Out Of This

<img
  src='/post/2019-08-23-mesh-reduction-1_files/images/harl-small.png'
  data-src-big='/post/2019-08-23-mesh-reduction-1_files/images/harl.png'
  data-caption='Tolerance 5%, Triangle Faces Covered By Approximation Level'
  class='post-inset-image bgw-zoom-img'
/>

So what are we #rstats users to do?  Well, as [&commat;mdsumner][13]
[enthusiastically puts it][7]:

> JavaScript is cool, C++ is cool. But #rstats can vectorize the absolute shit
> out of this. No loops until triangle realization
>
> -- @mdsumner

Indeed, we don't need to watch the [algorithm play out](#mesh-anim) very long to
see that its elegance comes at the expense of computation.  Finding each
triangle requires many steps.  It should be possible to compute each triangle's
coordinates directly and independently of the others, which means we should be
able to vectorize the process.

So, let's go vectorize the absolute $#!+ out of this.  It'll be fun!  If you're
into that type of thing anyway.  Stay tuned for RTINI Part II.

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

Special thanks to [Tyler Morgan Wall][15] for [`rayrender`][16] with which we
made many of the illustrations in this post, and for several design cues I
borrowed from his blog.  Special thanks also to [Vladimir Agafonkin][5] for
writing great posts and for introducing me to the RTIN algorithm.

The following  are post-specific acknowledgments.  This website owes many
additional thanks to [generous people and organizations][260] that have made it
possible.

* [R-core](https://www.r-project.org/contributors.html) for creating and
  maintaining a language so wonderful it allows crazy things like
  self-instrumentation out of the box.
* [Thomas Lin Pedersen][300] for [`gganimate`][310] with which I prototyped some
  of the earlier animations, and the FFmpeg team for [FFmpeg][270] with which I
  stitched the frames off the videos in this post.
* [Hadley Wickham][202] and the [`ggplot2` authors][203] for `ggplot2` with
  which I made many the plots in this post.
* [Hadley Wickham][202] etal. for [`reshape2`][14], and `dplyr::bind_rows`.
* Simon Urbanek for the [PNG
  package](https://cran.r-project.org/web/packages/png/index.html) which I used
  while post-processing many of the images in this post.
* [Oleg Sklyar][123], [Dirk Eddelbuettel][122], [Romain François][121], etal. for
  [`inline`][410] for easy integration of C code into ad-hoc R functions.
* [Cynthia Brewer][390] for color brewer palettes, [one of which][380] I used in
  some plots, and [axismaps][400] for the web tool for picking them.

## R Transliteration

This a reasonably close transliteration of the original [&commat;mourner][5]
[implementation][6] of the [RTIN error computation algorithm][8].  In addition
to the conversion to R, I have made some mostly non-semantic changes so that the
code is easier to instrument, and so that the corresponding visualization occurs
more naturally.  In particular, variables are initialized to NA so they won't
plot until set.  This means the corners of the error matrix remain NA as those
are not computed.

This only computes the error.  Separate code is required to extract meshes.

```{r echo=FALSE}
source(
  '../../static/script/mesh-viz/rtin2.R', deparseCtrl='all', keep.source=TRUE
)
```
```{r}
errors_rtin2
```

## C Transliteration

```{r}
errors_rtin_c <- inline::cfunction(sig=c(terr='numeric', grid='integer'), body="
  int gridSize = asInteger(grid);
  int tileSize = gridSize - 1;
  SEXP errSxp = PROTECT(allocVector(REALSXP, gridSize * gridSize));
  double * terrain = REAL(terr);
  double * errors = REAL(errSxp);
  errors[0] = errors[gridSize - 1] = errors[gridSize * gridSize - 1] =
    errors[gridSize * gridSize - gridSize] = 0;

  int numSmallestTriangles = tileSize * tileSize;
  // 2 + 4 + 8 + ... 2^k = 2 * 2^k - 2
  int numTriangles = numSmallestTriangles * 2 - 2;
  int lastLevelIndex = numTriangles - numSmallestTriangles;

  // iterate over all possible triangles, starting from the smallest level
  for (int i = numTriangles - 1; i >= 0; i--) {

    // get triangle coordinates from its index in an implicit binary tree
    int id = i + 2;
    int ax = 0, ay = 0, bx = 0, by = 0, cx = 0, cy = 0;
    if (id & 1) {
      bx = by = cx = tileSize; // bottom-left triangle
    } else {
      ax = ay = cy = tileSize; // top-right triangle
    }
    while ((id >>= 1) > 1) {
      int mx = (ax + bx) >> 1;
      int my = (ay + by) >> 1;

      if (id & 1) { // left half
        bx = ax; by = ay;
        ax = cx; ay = cy;
      } else { // right half
        ax = bx; ay = by;
        bx = cx; by = cy;
      }
      cx = mx; cy = my;
    }
    // calculate error in the middle of the long edge of the triangle
    double interpolatedHeight =
      (terrain[ay * gridSize + ax] + terrain[by * gridSize + bx]) / 2;
    int middleIndex = ((ay + by) >> 1) * gridSize + ((ax + bx) >> 1);
    double middleError = abs(interpolatedHeight - terrain[middleIndex]);

    if (i >= lastLevelIndex) { // smallest triangles
      errors[middleIndex] = middleError;

    } else { // bigger triangles; accumulate error with children
      double leftChildError =
        errors[((ay + cy) >> 1) * gridSize + ((ax + cx) >> 1)];
      double rightChildError =
        errors[((by + cy) >> 1) * gridSize + ((bx + cx) >> 1)];

      double tmp = errors[middleIndex];
      tmp = tmp > middleError ? tmp : middleError;
      tmp = tmp > leftChildError ? tmp : leftChildError;
      tmp = tmp > rightChildError ? tmp : rightChildError;
      errors[middleIndex] = tmp;
    }
  }
  UNPROTECT(1);
  return errSxp;
")
```


## System Info

```{r child='../../static/script/_lib/zoom-img/zoom-img.Rmd', results='asis', cache=FALSE}
```
```{r child='../../static/script/_lib/flipbook/flipbook.Rmd', results='asis', cache=FALSE}
```
<script type='text/javascript'>
const imgDir = '/post/2019-08-23-mesh-reduction-1_files/images/flipbook/';
const fps = 8;
new BgFlipBook({
  targetId: 'flipbook1', imgDir:imgDir, imgStart: 7, imgEnd: 50,
  imgPad: "0000", fps: fps, loop: false
})
new BgFlipBook({
  targetId: 'flipbook2', imgDir:imgDir, imgStart: 803, imgEnd: 844,
  imgPad: "0000", fps: fps, loop: false
})
</script>


[^off-course]: The beauty is I wasn't going anywhere in particular anyway, so
  why not do this?
[^volcano-size]: Yes, yes, `volcano` is not really 65 x 65, more on that in the
  next post.
[^transliterated]: Meaning we copied it into R with the minimal set of changes
  required to get it to work.
[^min-complex]: A 5x5 mesh is the smallest mesh that clearly showcases the
  complexity of the algorithm.
[^no-child-error]: We can split this triangle once again, but at that level
  every point on the elevation map coincides with a vertex so there is no error.
[^second-triangle]: The first triangle is not very interesting because the child
  errors are smaller than the parent error so there is no error to carry over.
[^timings-note]: Chrome was 2.5x faster than Firefox, and tended to get
  substantially faster after the first few runs, presumably due to adaptive
  compilation.  The reported times are after the timings stabilized, so will
  overstate the speed of the initial run.


[5]: https://twitter.com/mourner
[6]: https://observablehq.com/@mourner/martin-real-time-rtin-terrain-mesh
[7]: https://twitter.com/mdsumner/status/1161994475184373761?s=20
[8]: https://www.cs.ubc.ca/~will/papers/rtin.pdf
[9]: /2019/10/30/visualizing-algorithms/
[10]: https://twitter.com/BrodieGaslam/status/1166885035489804289?s=20
[11]: https://github.com/brodieG/rtini/blob/v0.1.0/R/error.R#L98
[12]: https://en.wikipedia.org/wiki/Louvre_Pyramid
[13]: https://twitter.com/mdsumner
[14]: https://cran.r-project.org/web/packages/V8/
[15]: http://www.rayrender.net/
[16]: https://twitter.com/tylermorganwall

[121]: https://github.com/romainfrancois
[122]: https://github.com/eddelbuettel
[123]: https://github.com/osklyar

[202]: https://github.com/hadley
[203]: https://cran.r-project.org/web/packages/ggplot2/index.html
[260]: /about/#acknowledgments
[270]: http://ffmpeg.org/about.html
[300]: https://twitter.com/thomasp85
[310]: https://github.com/thomasp85/gganimate

[390]: http://www.personal.psu.edu/cab38/
[400]: https://www.axismaps.com/
[410]: https://github.com/eddelbuettel/inline

